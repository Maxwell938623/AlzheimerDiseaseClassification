import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor

# Define the CNN model
class CNN(nn.Module):
    def __init__(self, num_filters=32, kernel_size=3):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, num_filters, kernel_size)
        self.pool = nn.MaxPool2d(2)
        self.fc = nn.Linear(num_filters * 13 * 13, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# Load the MNIST dataset
train_dataset = MNIST(root='./data', train=True, transform=ToTensor(), download=True)
test_dataset = MNIST(root='./data', train=False, transform=ToTensor())

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Define the hyperparameter search space
param_grid = {
    'num_filters': [16, 32, 64],
    'kernel_size': [3, 5],
    'learning_rate': [0.001, 0.01, 0.1]
}

# Define the training loop
def train(model, optimizer, criterion, dataloader):
    model.train()
    for images, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Define the evaluation function
def evaluate(model, dataloader):
    model.eval()
    predictions = []
    true_labels = []
    with torch.no_grad():
        for images, labels in dataloader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            predictions.extend(predicted.tolist())
            true_labels.extend(labels.tolist())
    accuracy = accuracy_score(true_labels, predictions)
    return accuracy

# Create the CNN model
model = CNN()

# Define the criterion and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Perform grid search
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy')

# Convert data to tensors
X_train = torch.stack([data[0] for data in train_dataset])
y_train = torch.tensor([data[1] for data in train_dataset])

# Perform grid search
grid_result = grid.fit(X_train, y_train)

# Print the best hyperparameters and accuracy
print("Best Hyperparameters: ", grid_result.best_params_)
print("Best Accuracy: ", grid_result.best_score_)

# Evaluate the model with the best hyperparameters on the test set
best_model = grid_result.best_estimator_
X_test = torch.stack([data[0] for data in test_dataset])
y_test = torch.tensor([data[1] for data in test_dataset])
y_pred = best_model(X_test)
accuracy = accuracy_score(y_test, y_pred.argmax(dim=1))
print("Test Accuracy: ", accuracy)
