{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install matplotlib\\n!pip install numpy\\n!pip install pandas\\n!pip install seaborn\\n!pip install torch\\n!pip install scikit-learn\\n!pip install torchvision\\n!pip install tqdm'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install torchvision\n",
    "!pip install tqdm\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "dml = torch_directml.device()\n",
    "print(dml)\n",
    "tensor1 = torch.tensor([1]).to(dml) # Note that dml is a variable, not a string!\n",
    "tensor2 = torch.tensor([2]).to(dml)\n",
    "dml_algebra = tensor1 + tensor2\n",
    "print(dml_algebra.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s_LcLuH7bIzx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcBtS1tmbayB"
   },
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "--VKWUDEbYOO"
   },
   "outputs": [],
   "source": [
    "class Hparams:\n",
    "    def __init__(self, train_batch_size=64, test_batch_size=64, learning_rate=0.001, num_epochs = 10, val_split=0.2, test_split=0.2, model_path='saved_model', dataset_path='Data'):\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        self.model_path = model_path\n",
    "        self.dataset_path = dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWmk5AcYbgOi"
   },
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8FsXSF1Xbcoc"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset = None, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            #mean = torch.mean(x)\n",
    "            #std = torch.std(x)\n",
    "        #return x, mean, std, y\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xqIsDN5jbqTg"
   },
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((248, 248)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_train():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((248, 248)),\n",
    "        transforms.RandomAffine(0, translate=None, scale= (0.8, 1.2), shear=None),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JX_WOjNEbsZO"
   },
   "outputs": [],
   "source": [
    "def get_sample_weights(dataset, train_dataset):\n",
    "\n",
    "    # Code taken from:\n",
    "    #     https://www.maskaravivek.com/post/pytorch-weighted-random-sampler/\n",
    "    y_train_indices = train_dataset.indices\n",
    "    print(train_dataset.indices)\n",
    "    print(dataset.targets)\n",
    "    y_train = [dataset.targets[i] for i in y_train_indices]\n",
    "\n",
    "    class_sample_counts = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "\n",
    "    weights = 1. / class_sample_counts\n",
    "    sample_weights = np.array([weights[t] for t in y_train])\n",
    "    sample_weights = torch.from_numpy(sample_weights)\n",
    "\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TF__YljXbvI1"
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(hparams):\n",
    "    # Loading the dataset\n",
    "    dataset = datasets.ImageFolder(hparams.dataset_path,\n",
    "                                   transform=transforms.Compose([transforms.Grayscale()]))\n",
    "\n",
    "    # Splitting dataset into train, validation and test partitions.\n",
    "    proportions = [(1 - hparams.val_split - hparams.test_split), hparams.val_split, hparams.test_split]\n",
    "    lengths = [int(p * len(dataset)) for p in proportions]\n",
    "    lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "\n",
    "    print(type(train_dataset))\n",
    "\n",
    "    print(type(train_dataset[0][0]))\n",
    "\n",
    "    print(f'train size: {lengths[0]}, val size: {lengths[1]}, test size: {lengths[2]}')\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': get_transforms_train(),\n",
    "        'test': get_transforms()\n",
    "    }\n",
    "\n",
    "    # Using WeightedRandomSampler to overcome unbalance problem\n",
    "    sample_weights = get_sample_weights(dataset, train_dataset)\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights.type('torch.DoubleTensor'), len(sample_weights))\n",
    "\n",
    "    train_dataset = CustomDataset(train_dataset, transform=data_transforms['train'])\n",
    "    val_dataset = CustomDataset(val_dataset, transform=data_transforms['test'])\n",
    "    test_dataset = CustomDataset(test_dataset, transform=data_transforms['test'])\n",
    "\n",
    "    plt.imshow(train_dataset[0][0].moveaxis(0,2), cmap='gray') \n",
    "\n",
    "    # Creating loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams.train_batch_size, sampler=train_sampler, drop_last=True, shuffle = True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams.train_batch_size, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams.test_batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_split_dataset_loaders(hparams):\n",
    "\n",
    "    folder_path = 'Data\\\\Mild Dementia'\n",
    "\n",
    "    error = 3\n",
    "\n",
    "    # Get a list of all files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    image_names = [file for file in file_list]\n",
    "\n",
    "    for i in range (len(image_names)):\n",
    "        image_names[i] = image_names[i][:-14]\n",
    "\n",
    "    uniqueID = pd.Series(image_names)\n",
    "    uniqueID = uniqueID.unique()\n",
    "\n",
    "    # Proportions you want to split into (e.g., 60% train, 20% validation, 20% test)\n",
    "    train_proportion = 1 - hparams.val_split - hparams.test_split\n",
    "    validation_proportion = hparams.val_split\n",
    "    test_proportion = hparams.test_split\n",
    "\n",
    "    # Perform the random split\n",
    "    train_ID, temp_ID = train_test_split(uniqueID, train_size=train_proportion, random_state=42)\n",
    "    validation_ID, test_ID = train_test_split(temp_ID, train_size=validation_proportion / (validation_proportion + test_proportion), random_state=42)\n",
    "\n",
    "    dataset = datasets.ImageFolder(hparams.dataset_path,\n",
    "                                   transform=transforms.Compose([transforms.Grayscale()]))\n",
    "\n",
    "    index = 0\n",
    "    location_change = [0]\n",
    "    for i in range(len(dataset)):\n",
    "        if (dataset[i][1] != index):\n",
    "            location_change.append(i)\n",
    "            index += 1\n",
    "\n",
    "    train_index = []\n",
    "    for i in range (len(train_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (train_ID[i] in file_list[ii]):\n",
    "                if (filter_out(23, dataset[ii+location_change[0]][0])):\n",
    "                    train_index.append(ii+location_change[0])\n",
    "            \n",
    "    val_index = []\n",
    "    for i in range (len(validation_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (validation_ID[i] in file_list[ii]):\n",
    "                if (filter_out(23, dataset[ii+location_change[0]][0])):\n",
    "                    val_index.append(ii+location_change[0])\n",
    "\n",
    "    test_index = []\n",
    "    for i in range (len(test_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (test_ID[i] in file_list[ii]):\n",
    "                test_index.append(ii+location_change[0])\n",
    "\n",
    "\n",
    "    folder_path = 'Data\\\\Regular'\n",
    "\n",
    "    # Get a list of all files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    image_names = [file for file in file_list]\n",
    "\n",
    "    for i in range (len(image_names)):\n",
    "        image_names[i] = image_names[i][:-14]\n",
    "\n",
    "    uniqueID = pd.Series(image_names)\n",
    "    uniqueID = uniqueID.unique()\n",
    "\n",
    "    # Perform the random split\n",
    "    train_ID, temp_ID = train_test_split(uniqueID, train_size=train_proportion, random_state=42)\n",
    "    validation_ID, test_ID = train_test_split(temp_ID, train_size=validation_proportion / (validation_proportion + test_proportion), random_state=42)\n",
    "\n",
    "    for i in range (len(train_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (train_ID[i] in file_list[ii]):\n",
    "                if (filter_out(23, dataset[ii+location_change[0]][0])):\n",
    "                    train_index.append(ii+location_change[1])\n",
    "            \n",
    "    for i in range (len(validation_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (validation_ID[i] in file_list[ii]):\n",
    "                if (filter_out(23, dataset[ii+location_change[0]][0])):\n",
    "                    val_index.append(ii+location_change[1])\n",
    "\n",
    "    for i in range (len(test_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (test_ID[i] in file_list[ii]):\n",
    "                test_index.append(ii+location_change[1])\n",
    "\n",
    "\n",
    "    folder_path = 'Data\\\\Very mild Dementia'\n",
    "\n",
    "    # Get a list of all files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    image_names = [file for file in file_list]\n",
    "\n",
    "    for i in range (len(image_names)):\n",
    "        image_names[i] = image_names[i][:-14]\n",
    "\n",
    "    uniqueID = pd.Series(image_names)\n",
    "    uniqueID = uniqueID.unique()\n",
    "\n",
    "    # Perform the random split\n",
    "    train_ID, temp_ID = train_test_split(uniqueID, train_size=train_proportion, random_state=42)\n",
    "    validation_ID, test_ID = train_test_split(temp_ID, train_size=validation_proportion / (validation_proportion + test_proportion), random_state=42)\n",
    "\n",
    "    for i in range (len(train_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (train_ID[i] in file_list[ii]):\n",
    "                if (filter_out(23, dataset[ii+location_change[0]][0])):\n",
    "                    train_index.append(ii+location_change[2])     \n",
    "\n",
    "    for i in range (len(validation_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (validation_ID[i] in file_list[ii]):\n",
    "                if (filter_out(23, dataset[ii+location_change[0]][0])):\n",
    "                    val_index.append(ii+location_change[2])\n",
    "\n",
    "    for i in range (len(test_ID)):\n",
    "        for ii in range (len(file_list)):\n",
    "            if (test_ID[i] in file_list[ii]):\n",
    "                test_index.append(ii+location_change[2])\n",
    "\n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    val_dataset = Subset(dataset, val_index)\n",
    "    test_dataset =  Subset(dataset, test_index)\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': get_transforms_train(),\n",
    "        'test': get_transforms()\n",
    "    }\n",
    "\n",
    "    # Using WeightedRandomSampler to overcome unbalance problem\n",
    "    sample_weights = get_sample_weights(dataset, train_dataset)\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights.type('torch.DoubleTensor'), len(sample_weights))\n",
    "\n",
    "    train_dataset = CustomDataset(train_dataset, transform=data_transforms['train'])\n",
    "    val_dataset = CustomDataset(val_dataset, transform=data_transforms['test'])\n",
    "    test_dataset = CustomDataset(test_dataset, transform=data_transforms['test'])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams.train_batch_size, sampler=train_sampler, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams.train_batch_size, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams.test_batch_size)\n",
    "\n",
    "    print(check_array_overlap(train_index, val_index, test_index))\n",
    "\n",
    "    print(len(train_dataset),len(val_dataset),len(test_dataset))\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_array_overlap(arr1, arr2, arr3):\n",
    "    set1 = set(arr1)\n",
    "    set2 = set(arr2)\n",
    "    set3 = set(arr3)\n",
    "    print(set1.intersection(set2))\n",
    "    print(set2.intersection(set3))\n",
    "    print(set3.intersection(set1))\n",
    "\n",
    "    if set1.intersection(set2) or set1.intersection(set3) or set2.intersection(set3):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out(min_threshold,image):\n",
    "    \n",
    "    grayscale_img = image.convert('L')\n",
    "\n",
    "    grayscale_img = grayscale_img.filter(ImageFilter.GaussianBlur(1))\n",
    "\n",
    "    enhancer = ImageEnhance.Contrast(grayscale_img)\n",
    "\n",
    "    # Enhance the contrast of the image\n",
    "    grayscale_img = enhancer.enhance(8)\n",
    "\n",
    "    # Initialize a counter for mostly black pixels\n",
    "    mostly_black_count = 0\n",
    "\n",
    "    # Get the width and height of the image\n",
    "    width, height = grayscale_img.size\n",
    "\n",
    "    area = width*height\n",
    "\n",
    "    flipped = False\n",
    "\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            pixel_value = grayscale_img.getpixel((i, j))\n",
    "            if (i > width/2):\n",
    "                grayscale_img.putpixel((i, j), 255)\n",
    "                area -= 1\n",
    "            elif pixel_value < 40:\n",
    "                grayscale_img.putpixel((i, j), 255)\n",
    "                area -= 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    for i in range(width):\n",
    "        for j in range(height-1, -1, -1):\n",
    "            pixel_value = grayscale_img.getpixel((i, j))\n",
    "            if (i > width/2 and pixel_value != 255):\n",
    "                grayscale_img.putpixel((i, j), 255)\n",
    "                area -= 1\n",
    "            elif pixel_value < 40:\n",
    "                grayscale_img.putpixel((i, j), 255)\n",
    "                area -=1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel_value = grayscale_img.getpixel((x, y))\n",
    "            if pixel_value < 40:\n",
    "                if (59 < y and y < 172 and 149 < x and x < 313):\n",
    "                    #mostly_black_count += 0\n",
    "                    mostly_black_count += 1\n",
    "                else:\n",
    "                    mostly_black_count += 1\n",
    "\n",
    "    percentage = (mostly_black_count/area)*100\n",
    "\n",
    "    if percentage > min_threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9E5SfW4cXUC"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qKy1dYhjcYtJ"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, mid_channel, out_channel, kernel= (5,5)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, mid_channel, kernel_size= kernel, padding = 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(mid_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(mid_channel, out_channel, kernel_size= kernel, padding = 1)\n",
    "        #self.conv2 = nn.Conv2d(in_channel, out_channel, kernel_size= kernel, padding = 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channel)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UL6y4BYwdJJz"
   },
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_channel, out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm1d(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlockDropout(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_channel, out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm1d(out_channel)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mPK59l7Pd4nA"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convblock1 = ConvBlock(1, 2, 4)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.linearblock1 = LinearBlockDropout(59536, 32)\n",
    "        #self.linearblock4 = LinearBlock(32 + 2, 3)\n",
    "        self.linearblock4 = LinearBlock(32, 3)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    #def forward(self, img, mean, std):\n",
    "    def forward(self, img):\n",
    "        x = self.convblock1(img)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linearblock1(x)\n",
    "\n",
    "        x = self.linearblock4(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3oNQSTheZEQ"
   },
   "source": [
    "Train and Validate Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "J-VmmirpeaW3"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    batch_size = 0\n",
    "\n",
    "    targets, preds = [], []\n",
    "\n",
    "    #for batch_idx, (img, mean, std, target) in enumerate(train_loader):\n",
    "    #    img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
    "    for batch_idx, (img, target) in train_loader:\n",
    "        img, target = img.to(device), target.to(device) \n",
    "        batch_size = len(img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #output = model(img, mean, std)\n",
    "        output = model(img)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        targets.append(target.cpu().numpy())\n",
    "        preds.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loader.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        train_loader.set_postfix(loss=train_loss / ((batch_idx+1) * len(img)), accuracy=100. * train_correct / ((batch_idx+1) * len(img)))\n",
    "\n",
    "    targets = np.concatenate(targets)\n",
    "    preds = np.concatenate(preds)\n",
    "    f1 = f1_score(targets, preds, average='macro')\n",
    "\n",
    "    train_length = train_loader.total * batch_size\n",
    "    train_loss /= train_length\n",
    "    train_accuracy = 100. * train_correct / train_length\n",
    "    return train_loss, train_accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WoYOtELfejvh"
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    total_size = 0\n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (img, mean, std, target) in enumerate(val_loader):\n",
    "        #    img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
    "        for batch_idx, (img, target) in enumerate(val_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            batch_size = len(img)\n",
    "        #    output = model(img, mean, std)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            total_size += len(img)\n",
    "    val_loss /= total_size\n",
    "    val_accuracy = 100. * val_correct / total_size\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NUp6gKkKeke7"
   },
   "outputs": [],
   "source": [
    "def predict(model, dataset, criterion, device, eval=False):\n",
    "    model.eval()\n",
    "    pred_loss = 0\n",
    "    pred_correct = 0\n",
    "    total_size = 0\n",
    "\n",
    "    predictions = torch.IntTensor()\n",
    "    ground_truths = torch.IntTensor()\n",
    "\n",
    "    predictions, ground_truths = predictions.to(device), ground_truths.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (img, mean, std, target) in dataset:\n",
    "            #img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
    "            #output = model(img, mean, std)\n",
    "        for batch_idx, (img, target) in enumerate(dataset):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, target)\n",
    "            pred_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            pred_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            predictions = torch.cat((predictions, pred), dim=0)\n",
    "            ground_truths = torch.cat((ground_truths, target), dim=0)\n",
    "\n",
    "            total_size += len(img)\n",
    "\n",
    "    pred_loss /= total_size\n",
    "    pred_accuracy = 100. * pred_correct / total_size\n",
    "\n",
    "    if eval:\n",
    "        return pred_loss, pred_accuracy, predictions.cpu().numpy(), ground_truths.cpu().numpy()\n",
    "    else:\n",
    "        return predictions.cpu().numpy(), ground_truths.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NQBoutrhgbhj"
   },
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_dataset, test_loader, criterion, optimizer, device, num_epochs, early_stopping=None):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader), ascii=' >=')\n",
    "        train_loss, train_accuracy, f1 = train(model, loop, criterion, optimizer, device, epoch, num_epochs)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        val_loss, val_accuracy = validate(model, val_dataset, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "#        predictions, labels = predict(model, val_loader, criterion, device, eval=False)\n",
    "\n",
    "#        conf_mat = confusion_matrix(labels, predictions)\n",
    "#        class_to_idx = list(train_loader.dataset.subset.dataset.class_to_idx)\n",
    "#        df_cm = pd.DataFrame(conf_mat, index = class_to_idx, columns = class_to_idx)\n",
    "#        heat_map = sn.heatmap(df_cm, annot=True, fmt='', cmap='Blues')\n",
    "#        plt.show()\n",
    "#\n",
    "#        for i in range(len(predictions)):\n",
    "#            if (predictions[i] != labels[i]):\n",
    "#                print(f'Image: {val_loader.dataset.subset.dataset.imgs[i][0]}, Predicted: {class_to_idx[int(predictions[i])]}, Actual: {class_to_idx[int(labels[i])]}')\n",
    "        \n",
    "        if early_stopping is not None:\n",
    "            early_stopping(val_accuracy)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                tqdm.write(f'\\t => train_f1={f1:.4f}, val_loss={val_loss:.4f}, val_acc={val_accuracy:.4f}')\n",
    "                print(f'Early stopping at Epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "        tqdm.write(f'\\t => train_f1={f1:.4f}, val_loss={val_loss:.4f}, val_acc={val_accuracy:.4f}')\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictspecial(model, dataset, criterion, device):\n",
    "    model.eval()\n",
    "    pred_loss_brain = 0\n",
    "    pred_loss = 0\n",
    "    pred_correct = 0\n",
    "    total_size = 0\n",
    "\n",
    "    accuracy_array = []\n",
    "\n",
    "    predictions = torch.IntTensor()\n",
    "    ground_truths = torch.IntTensor()\n",
    "\n",
    "    predictions, ground_truths = predictions.to(device), ground_truths.to(device)\n",
    "\n",
    "    for index1 in range(len(dataset)//61):\n",
    "        raw_predictions_sum = [0,0,0]\n",
    "        target_overall = 0\n",
    "        for index2 in range(61):\n",
    "            #img, mean, std, target = dataset[index1*61+index2]\n",
    "            img, target = dataset[index1*61+index2][0], dataset[index1*61+index2][1]\n",
    "            img = img.unsqueeze(0)\n",
    "            target_overall = target\n",
    "            #target = torch.tensor(target, dtype=torch.int8).unsqueeze(0).to(device)\n",
    "            #print(target.shape)\n",
    "            #img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
    "            img = img.to(device)\n",
    "            #output = model(img, mean, std)\n",
    "            output = model(img)\n",
    "            #loss = criterion(output, target)\n",
    "            #pred_loss_brain += loss.item()\n",
    "            pred_brain = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            if (pred_brain == 0):\n",
    "                raw_predictions_sum[2] += 1\n",
    "            elif (pred_brain == 1):\n",
    "                raw_predictions_sum[0] += 1\n",
    "            else:\n",
    "                raw_predictions_sum[1] += 1\n",
    "\n",
    "            total_size += len(img)\n",
    "\n",
    "        pred_loss += pred_loss_brain\n",
    "        \n",
    "        #predictions = torch.cat((predictions, pred), dim=0)\n",
    "        #ground_truths = torch.cat((ground_truths, target), dim=0)\n",
    "\n",
    "        accuracy_array.append((raw_predictions_sum, target_overall))\n",
    "\n",
    "    pred_loss /= total_size\n",
    "    pred_accuracy = 100. * pred_correct / (len(dataset)//61)\n",
    "    \n",
    "    return accuracy_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0BxduGagdzF"
   },
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Q-EdFYJkgcWO"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, mode='max'):\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            self.ref_value = float('-inf')\n",
    "        elif self.mode == 'min':\n",
    "            self.ref_value = float('inf')\n",
    "        else:\n",
    "            raise Exception(f'Undefined mode for EarlyStopping - mode: {mode}\\n'\n",
    "                             'Available modes are [\"max\", \"min\"]')\n",
    "\n",
    "    def __call__(self, value):\n",
    "        if self.mode == 'max':\n",
    "            if value <= self.ref_value:\n",
    "                self.counter += 1\n",
    "            else:\n",
    "                self.counter = 0\n",
    "                self.ref_value = value\n",
    "        elif self.mode == 'min':\n",
    "            if value >= self.ref_value:\n",
    "                self.counter += 1\n",
    "            else:\n",
    "                self.counter = 0\n",
    "                self.ref_value = value\n",
    "\n",
    "        if self.counter == self.patience:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q02aHJmGggnP"
   },
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2Cdvw21rghgu"
   },
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1OQa36N2gi7F"
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(train_accuracies, val_accuracies):\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZBlabjZgkB5"
   },
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "RuuEVoLygmJT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using privateuseone:0 device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch_directml.device()\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rf8Rt7S0gnYE"
   },
   "outputs": [],
   "source": [
    "hparams = Hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jbnuuRwggooM"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader, val_loader, test_loader, train_dataset , val_dataset , test_dataset \u001b[39m=\u001b[39m get_split_dataset_loaders(hparams)\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mget_split_dataset_loaders\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     33\u001b[0m location_change \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)):\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mif\u001b[39;00m (dataset[i][\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m index):\n\u001b[0;32m     36\u001b[0m         location_change\u001b[39m.\u001b[39mappend(i)\n\u001b[0;32m     37\u001b[0m         index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[0;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(f)\n\u001b[0;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\PIL\\Image.py:3164\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3156\u001b[0m     \u001b[39mif\u001b[39;00m pixels \u001b[39m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[0;32m   3157\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   3158\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mImage size (\u001b[39m\u001b[39m{\u001b[39;00mpixels\u001b[39m}\u001b[39;00m\u001b[39m pixels) exceeds limit of \u001b[39m\u001b[39m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[39m}\u001b[39;00m\u001b[39m pixels, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3159\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcould be decompression bomb DOS attack.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3160\u001b[0m             DecompressionBombWarning,\n\u001b[0;32m   3161\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(fp, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, formats\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   3165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3166\u001b[0m \u001b[39m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[0;32m   3167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3191\u001b[0m \u001b[39m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[0;32m   3192\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3194\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, train_dataset , val_dataset , test_dataset = get_split_dataset_loaders(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ogRcj7hGgpuC"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=3, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCBqyVEpgq9Y"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "trk2nAuagr8J"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_WXpE2C0gtrj"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMBOCtnCguuS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: 100%|==========| 214/214 [01:16<00:00,  2.78it/s, accuracy=50.8, loss=0.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.5058, val_loss=0.0199, val_acc=15.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/30]: 100%|==========| 214/214 [01:05<00:00,  3.25it/s, accuracy=64.7, loss=0.0144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.6406, val_loss=0.0170, val_acc=40.2232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/30]: 100%|==========| 214/214 [01:10<00:00,  3.02it/s, accuracy=70.6, loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.7021, val_loss=0.0172, val_acc=38.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/30]: 100%|==========| 214/214 [01:09<00:00,  3.10it/s, accuracy=72.8, loss=0.0131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.7248, val_loss=0.0146, val_acc=59.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/30]: 100%|==========| 214/214 [01:12<00:00,  2.95it/s, accuracy=74.7, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.7454, val_loss=0.0154, val_acc=51.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/30]: 100%|==========| 214/214 [01:12<00:00,  2.94it/s, accuracy=77.3, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.7707, val_loss=0.0155, val_acc=51.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/30]: 100%|==========| 214/214 [01:17<00:00,  2.77it/s, accuracy=78.8, loss=0.0121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.7863, val_loss=0.0167, val_acc=44.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/30]: 100%|==========| 214/214 [01:23<00:00,  2.57it/s, accuracy=79.4, loss=0.012] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.7926, val_loss=0.0148, val_acc=57.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/30]: 100%|==========| 214/214 [01:22<00:00,  2.59it/s, accuracy=81.3, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.8106, val_loss=0.0138, val_acc=64.5982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/30]: 100%|==========| 214/214 [01:39<00:00,  2.16it/s, accuracy=82.3, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t => train_f1=0.8212, val_loss=0.0145, val_acc=60.5580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/30]:   8%|>         | 17/214 [00:06<01:19,  2.48it/s, accuracy=83.2, loss=0.0114]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[39m=\u001b[39m train_and_validate(model,\n\u001b[0;32m      2\u001b[0m                                                                                 train_loader,\n\u001b[0;32m      3\u001b[0m                                                                                 val_loader,\n\u001b[0;32m      4\u001b[0m                                                                                 test_loader,\n\u001b[0;32m      5\u001b[0m                                                                                 criterion,\n\u001b[0;32m      6\u001b[0m                                                                                 optimizer,\n\u001b[0;32m      7\u001b[0m                                                                                 device,\n\u001b[0;32m      8\u001b[0m                                                                                 num_epochs\u001b[39m=\u001b[39;49mhparams\u001b[39m.\u001b[39;49mnum_epochs,\n\u001b[0;32m      9\u001b[0m                                                                                 \u001b[39m#early_stopping=early_stopping,\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m                                                                                 )\n",
      "Cell \u001b[1;32mIn[188], line 9\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_dataset, test_loader, criterion, optimizer, device, num_epochs, early_stopping)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     loop \u001b[39m=\u001b[39m tqdm(\u001b[39menumerate\u001b[39m(train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader), ascii\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m >=\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     train_loss, train_accuracy, f1 \u001b[39m=\u001b[39m train(model, loop, criterion, optimizer, device, epoch, num_epochs)\n\u001b[0;32m     10\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     11\u001b[0m     train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n",
      "Cell \u001b[1;32mIn[200], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m     21\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 22\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     24\u001b[0m targets\u001b[39m.\u001b[39mappend(target\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     25\u001b[0m preds\u001b[39m.\u001b[39mappend(pred\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = train_and_validate(model,\n",
    "                                                                                train_loader,\n",
    "                                                                                val_loader,\n",
    "                                                                                test_loader,\n",
    "                                                                                criterion,\n",
    "                                                                                optimizer,\n",
    "                                                                                device,\n",
    "                                                                                num_epochs=hparams.num_epochs,\n",
    "                                                                                #early_stopping=early_stopping,\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEG_4O8ogwL1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_losses(train_losses, val_losses)\n\u001b[0;32m      2\u001b[0m plot_accuracies(train_accuracies, val_accuracies)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plot_losses(train_losses, val_losses)\n",
    "plot_accuracies(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaSeYMEtg0G7"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_gQXMEmg1EX"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m validate(model, test_loader, criterion, device)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = validate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, hparams):\n",
    "    os.makedirs(hparams.model_path, exist_ok=True)\n",
    "\n",
    "    model_name = model.__class__.__name__ + '_' + datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\" + '.pt')\n",
    "\n",
    "    try:\n",
    "        torch.save(model.state_dict(), os.path.join(hparams.model_path, model_name))\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUPmzNIxg3z_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_pt_file = \"saved_model\\Model 20 (Very Good)\\CNN_2023_07_21-22_52_21.pt\"\n",
    "\n",
    "saved_model = torch.load(path_to_pt_file)\n",
    "\n",
    "model.load_state_dict(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk_YKsApg6zM"
   },
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8tGvKerg78E"
   },
   "outputs": [],
   "source": [
    "#predictions, labels = predict(model, test_loader, criterion, device, eval=False)\n",
    "array = predictspecial(model, test_dataset, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 7],\n",
      "       [ 4],\n",
      "       [ 7],\n",
      "       [ 7],\n",
      "       [16],\n",
      "       [15],\n",
      "       [18],\n",
      "       [25],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [ 5],\n",
      "       [ 5],\n",
      "       [ 4],\n",
      "       [ 6],\n",
      "       [ 2],\n",
      "       [ 4],\n",
      "       [23],\n",
      "       [24],\n",
      "       [24],\n",
      "       [24],\n",
      "       [46],\n",
      "       [51],\n",
      "       [46],\n",
      "       [46],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [55],\n",
      "       [56],\n",
      "       [52],\n",
      "       [56],\n",
      "       [ 5],\n",
      "       [ 5],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [32],\n",
      "       [26],\n",
      "       [29],\n",
      "       [29],\n",
      "       [58],\n",
      "       [54],\n",
      "       [58],\n",
      "       [58],\n",
      "       [39],\n",
      "       [40],\n",
      "       [44],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [59],\n",
      "       [59],\n",
      "       [59],\n",
      "       [60],\n",
      "       [54],\n",
      "       [55],\n",
      "       [56],\n",
      "       [54],\n",
      "       [47],\n",
      "       [43],\n",
      "       [43],\n",
      "       [43],\n",
      "       [59],\n",
      "       [59],\n",
      "       [60],\n",
      "       [61],\n",
      "       [37],\n",
      "       [50],\n",
      "       [49],\n",
      "       [51],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [60],\n",
      "       [23],\n",
      "       [23],\n",
      "       [26],\n",
      "       [23],\n",
      "       [61],\n",
      "       [61],\n",
      "       [60],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [57],\n",
      "       [57],\n",
      "       [59],\n",
      "       [60],\n",
      "       [58],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [ 6],\n",
      "       [ 4],\n",
      "       [ 6],\n",
      "       [ 5],\n",
      "       [52],\n",
      "       [53],\n",
      "       [55],\n",
      "       [54],\n",
      "       [28],\n",
      "       [19],\n",
      "       [23],\n",
      "       [28],\n",
      "       [11],\n",
      "       [11],\n",
      "       [10],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [56],\n",
      "       [52],\n",
      "       [51],\n",
      "       [55],\n",
      "       [16],\n",
      "       [30],\n",
      "       [29],\n",
      "       [20],\n",
      "       [49],\n",
      "       [50],\n",
      "       [49],\n",
      "       [47],\n",
      "       [52],\n",
      "       [53],\n",
      "       [55],\n",
      "       [51],\n",
      "       [58],\n",
      "       [59],\n",
      "       [58],\n",
      "       [59],\n",
      "       [26],\n",
      "       [28],\n",
      "       [30],\n",
      "       [24],\n",
      "       [40],\n",
      "       [39],\n",
      "       [40],\n",
      "       [41],\n",
      "       [45],\n",
      "       [49],\n",
      "       [43],\n",
      "       [46],\n",
      "       [40],\n",
      "       [43],\n",
      "       [42],\n",
      "       [36],\n",
      "       [20],\n",
      "       [23],\n",
      "       [19],\n",
      "       [24],\n",
      "       [59],\n",
      "       [58],\n",
      "       [60],\n",
      "       [60],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [22],\n",
      "       [24],\n",
      "       [19],\n",
      "       [24],\n",
      "       [14],\n",
      "       [17],\n",
      "       [21],\n",
      "       [14],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 3],\n",
      "       [24],\n",
      "       [20],\n",
      "       [22],\n",
      "       [22],\n",
      "       [55],\n",
      "       [55],\n",
      "       [55],\n",
      "       [55],\n",
      "       [26],\n",
      "       [21],\n",
      "       [24],\n",
      "       [22],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [60],\n",
      "       [18],\n",
      "       [20],\n",
      "       [20],\n",
      "       [21],\n",
      "       [29],\n",
      "       [45],\n",
      "       [37],\n",
      "       [36],\n",
      "       [33],\n",
      "       [35],\n",
      "       [39],\n",
      "       [59],\n",
      "       [57],\n",
      "       [58],\n",
      "       [58],\n",
      "       [12],\n",
      "       [10],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [11],\n",
      "       [ 7],\n",
      "       [ 6],\n",
      "       [10],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [33],\n",
      "       [32],\n",
      "       [31],\n",
      "       [39],\n",
      "       [44],\n",
      "       [50],\n",
      "       [52],\n",
      "       [55],\n",
      "       [ 5],\n",
      "       [ 6],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 4],\n",
      "       [23],\n",
      "       [15],\n",
      "       [16],\n",
      "       [12],\n",
      "       [12],\n",
      "       [14],\n",
      "       [15],\n",
      "       [14],\n",
      "       [13],\n",
      "       [15],\n",
      "       [14],\n",
      "       [ 5],\n",
      "       [ 3],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [12],\n",
      "       [17],\n",
      "       [18],\n",
      "       [16],\n",
      "       [15],\n",
      "       [15],\n",
      "       [ 5],\n",
      "       [ 9],\n",
      "       [ 5],\n",
      "       [ 5],\n",
      "       [ 7],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 1]]), array([[ 0],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 7],\n",
      "       [13],\n",
      "       [10],\n",
      "       [19],\n",
      "       [ 8],\n",
      "       [ 7],\n",
      "       [ 8],\n",
      "       [10],\n",
      "       [17],\n",
      "       [13],\n",
      "       [11],\n",
      "       [12],\n",
      "       [14],\n",
      "       [10],\n",
      "       [14],\n",
      "       [14],\n",
      "       [10],\n",
      "       [13],\n",
      "       [16],\n",
      "       [10],\n",
      "       [ 5],\n",
      "       [ 5],\n",
      "       [ 9],\n",
      "       [ 5],\n",
      "       [14],\n",
      "       [20],\n",
      "       [18],\n",
      "       [12],\n",
      "       [20],\n",
      "       [25],\n",
      "       [21],\n",
      "       [21],\n",
      "       [ 2],\n",
      "       [ 6],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [10],\n",
      "       [14],\n",
      "       [11],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 2],\n",
      "       [14],\n",
      "       [18],\n",
      "       [18],\n",
      "       [18],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [24],\n",
      "       [11],\n",
      "       [11],\n",
      "       [10],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [34],\n",
      "       [34],\n",
      "       [35],\n",
      "       [36],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 4],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [ 3],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [40],\n",
      "       [38],\n",
      "       [39],\n",
      "       [43],\n",
      "       [ 6],\n",
      "       [ 5],\n",
      "       [ 6],\n",
      "       [ 5],\n",
      "       [28],\n",
      "       [35],\n",
      "       [35],\n",
      "       [29],\n",
      "       [43],\n",
      "       [39],\n",
      "       [40],\n",
      "       [ 7],\n",
      "       [ 6],\n",
      "       [ 9],\n",
      "       [ 5],\n",
      "       [26],\n",
      "       [29],\n",
      "       [18],\n",
      "       [ 2],\n",
      "       [ 5],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [39],\n",
      "       [25],\n",
      "       [29],\n",
      "       [36],\n",
      "       [12],\n",
      "       [11],\n",
      "       [12],\n",
      "       [14],\n",
      "       [ 3],\n",
      "       [ 2],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 2],\n",
      "       [27],\n",
      "       [21],\n",
      "       [28],\n",
      "       [32],\n",
      "       [ 9],\n",
      "       [ 9],\n",
      "       [10],\n",
      "       [12],\n",
      "       [ 9],\n",
      "       [ 6],\n",
      "       [ 7],\n",
      "       [ 6],\n",
      "       [17],\n",
      "       [16],\n",
      "       [16],\n",
      "       [18],\n",
      "       [10],\n",
      "       [ 9],\n",
      "       [ 7],\n",
      "       [ 8],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [33],\n",
      "       [32],\n",
      "       [33],\n",
      "       [34],\n",
      "       [47],\n",
      "       [44],\n",
      "       [37],\n",
      "       [47],\n",
      "       [44],\n",
      "       [46],\n",
      "       [46],\n",
      "       [45],\n",
      "       [33],\n",
      "       [38],\n",
      "       [35],\n",
      "       [36],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [10],\n",
      "       [10],\n",
      "       [ 7],\n",
      "       [ 8],\n",
      "       [ 3],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 2],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 7],\n",
      "       [10],\n",
      "       [10],\n",
      "       [17],\n",
      "       [21],\n",
      "       [ 7],\n",
      "       [14],\n",
      "       [20],\n",
      "       [23],\n",
      "       [22],\n",
      "       [22],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 2],\n",
      "       [32],\n",
      "       [30],\n",
      "       [37],\n",
      "       [34],\n",
      "       [ 6],\n",
      "       [ 4],\n",
      "       [ 4],\n",
      "       [ 4],\n",
      "       [45],\n",
      "       [45],\n",
      "       [39],\n",
      "       [41],\n",
      "       [ 3],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 5],\n",
      "       [ 7],\n",
      "       [ 5],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [12],\n",
      "       [15],\n",
      "       [17],\n",
      "       [ 1],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [20],\n",
      "       [25],\n",
      "       [22],\n",
      "       [23],\n",
      "       [21],\n",
      "       [27],\n",
      "       [23],\n",
      "       [22],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 6],\n",
      "       [ 5],\n",
      "       [ 5],\n",
      "       [ 6],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [15],\n",
      "       [13],\n",
      "       [18],\n",
      "       [11],\n",
      "       [12],\n",
      "       [11],\n",
      "       [ 4],\n",
      "       [11],\n",
      "       [13],\n",
      "       [13],\n",
      "       [ 8],\n",
      "       [15],\n",
      "       [22],\n",
      "       [22],\n",
      "       [16]]), array([[54],\n",
      "       [56],\n",
      "       [53],\n",
      "       [53],\n",
      "       [44],\n",
      "       [46],\n",
      "       [41],\n",
      "       [36],\n",
      "       [60],\n",
      "       [60],\n",
      "       [61],\n",
      "       [61],\n",
      "       [50],\n",
      "       [46],\n",
      "       [46],\n",
      "       [37],\n",
      "       [49],\n",
      "       [48],\n",
      "       [51],\n",
      "       [47],\n",
      "       [21],\n",
      "       [24],\n",
      "       [26],\n",
      "       [25],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [49],\n",
      "       [46],\n",
      "       [43],\n",
      "       [49],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [42],\n",
      "       [36],\n",
      "       [39],\n",
      "       [47],\n",
      "       [ 9],\n",
      "       [10],\n",
      "       [11],\n",
      "       [11],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 1],\n",
      "       [ 2],\n",
      "       [12],\n",
      "       [ 7],\n",
      "       [ 6],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 4],\n",
      "       [ 5],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 4],\n",
      "       [ 4],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [15],\n",
      "       [19],\n",
      "       [16],\n",
      "       [13],\n",
      "       [ 3],\n",
      "       [ 3],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 5],\n",
      "       [ 7],\n",
      "       [ 3],\n",
      "       [ 4],\n",
      "       [ 7],\n",
      "       [11],\n",
      "       [11],\n",
      "       [54],\n",
      "       [55],\n",
      "       [52],\n",
      "       [56],\n",
      "       [35],\n",
      "       [32],\n",
      "       [43],\n",
      "       [ 3],\n",
      "       [ 4],\n",
      "       [ 8],\n",
      "       [ 4],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 3],\n",
      "       [ 5],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 6],\n",
      "       [ 8],\n",
      "       [ 1],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 8],\n",
      "       [12],\n",
      "       [ 3],\n",
      "       [ 5],\n",
      "       [12],\n",
      "       [13],\n",
      "       [11],\n",
      "       [ 8],\n",
      "       [ 7],\n",
      "       [ 6],\n",
      "       [11],\n",
      "       [ 9],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [ 3],\n",
      "       [ 7],\n",
      "       [31],\n",
      "       [29],\n",
      "       [35],\n",
      "       [29],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [57],\n",
      "       [58],\n",
      "       [57],\n",
      "       [59],\n",
      "       [ 6],\n",
      "       [ 5],\n",
      "       [ 9],\n",
      "       [ 3],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 3],\n",
      "       [ 0],\n",
      "       [15],\n",
      "       [14],\n",
      "       [15],\n",
      "       [13],\n",
      "       [ 4],\n",
      "       [ 3],\n",
      "       [ 4],\n",
      "       [ 3],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [25],\n",
      "       [30],\n",
      "       [30],\n",
      "       [31],\n",
      "       [58],\n",
      "       [59],\n",
      "       [58],\n",
      "       [59],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       [36],\n",
      "       [31],\n",
      "       [31],\n",
      "       [23],\n",
      "       [11],\n",
      "       [ 9],\n",
      "       [10],\n",
      "       [ 5],\n",
      "       [ 5],\n",
      "       [ 4],\n",
      "       [ 0],\n",
      "       [ 2],\n",
      "       [ 4],\n",
      "       [ 2],\n",
      "       [ 2],\n",
      "       [48],\n",
      "       [49],\n",
      "       [52],\n",
      "       [53],\n",
      "       [18],\n",
      "       [24],\n",
      "       [18],\n",
      "       [17],\n",
      "       [54],\n",
      "       [56],\n",
      "       [57],\n",
      "       [55],\n",
      "       [16],\n",
      "       [16],\n",
      "       [22],\n",
      "       [20],\n",
      "       [25],\n",
      "       [27],\n",
      "       [28],\n",
      "       [20],\n",
      "       [14],\n",
      "       [ 6],\n",
      "       [ 2],\n",
      "       [ 1],\n",
      "       [56],\n",
      "       [55],\n",
      "       [59],\n",
      "       [58],\n",
      "       [60],\n",
      "       [61],\n",
      "       [60],\n",
      "       [57],\n",
      "       [26],\n",
      "       [31],\n",
      "       [28],\n",
      "       [48],\n",
      "       [47],\n",
      "       [45],\n",
      "       [45],\n",
      "       [27],\n",
      "       [23],\n",
      "       [24],\n",
      "       [24],\n",
      "       [35],\n",
      "       [31],\n",
      "       [32],\n",
      "       [33],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [61],\n",
      "       [54],\n",
      "       [55],\n",
      "       [56],\n",
      "       [55],\n",
      "       [61],\n",
      "       [60],\n",
      "       [59],\n",
      "       [61],\n",
      "       [34],\n",
      "       [31],\n",
      "       [25],\n",
      "       [34],\n",
      "       [34],\n",
      "       [35],\n",
      "       [52],\n",
      "       [41],\n",
      "       [43],\n",
      "       [43],\n",
      "       [46],\n",
      "       [45],\n",
      "       [39],\n",
      "       [38],\n",
      "       [44]]), array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]])]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def array_to_csv(array_data, csv_file_path):\n",
    "    with open(csv_file_path, mode='w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerows(array_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    temp = []\n",
    "    for i in range (len(array)):\n",
    "        if (array[i][1] == 1):\n",
    "            temp.append([array[i][0][0], array[i][0][1], array[i][0][2], 1])\n",
    "        else:\n",
    "            temp.append([array[i][0][0], array[i][0][1], array[i][0][2], 0])\n",
    "\n",
    "    two_column_array = np.array(temp)\n",
    "\n",
    "    column_wise_splits = np.hsplit(two_column_array, two_column_array.shape[1])\n",
    "\n",
    "    print(column_wise_splits)\n",
    "    \n",
    "    csv_file_path = \"output.csv\"  # Change this to the desired output file path\n",
    "    \n",
    "    array_to_csv(two_column_array, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6uOFQYXg9pk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Mild Dementia  Regular  Very mild Dementia\n",
      "Mild Dementia                 167       56                 377\n",
      "Regular                       369     3395                2956\n",
      "Very mild Dementia            646       37                 697\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(labels, predictions)\n",
    "class_to_idx = list(train_loader.dataset.subset.dataset.class_to_idx)\n",
    "df_cm = pd.DataFrame(conf_mat, index = class_to_idx, columns = class_to_idx)\n",
    "\n",
    "print(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_OeMZDNg_gY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.heatmap(df_cm, annot=True, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnZy8SoRhArB"
   },
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXSplxYChCE9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Mild Dementia       0.14      0.28      0.19       600\n",
      "           Regular       0.97      0.51      0.67      6720\n",
      "Very mild Dementia       0.17      0.51      0.26      1380\n",
      "\n",
      "          accuracy                           0.49      8700\n",
      "         macro avg       0.43      0.43      0.37      8700\n",
      "      weighted avg       0.79      0.49      0.57      8700\n",
      "\n",
      "[0 0 0 ... 2 2 2]\n",
      "[[2]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, predictions, target_names=class_to_idx))\n",
    "\n",
    "print(labels)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision():\n",
    "    precisions = []\n",
    "    for row in range(4):\n",
    "        precisions.append(df_cm[row][row] / (df_cm[0][row] + df_cm[1][row] + df_cm[2][row] + df_cm[3][row]))\n",
    "    \n",
    "    total_sum = 0\n",
    "    overall_precision = 0\n",
    "    for row in range(4):\n",
    "        row_sum = 0\n",
    "        for col in range(4):\n",
    "            row_sum += df_cm[row][col]\n",
    "        total_sum += row_sum\n",
    "        overall_precision += precisions[row] * row_sum\n",
    "    precisions.append(overall_precision / total_sum)\n",
    "    return precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall():\n",
    "    recalls = []\n",
    "    for col in range(4):\n",
    "        recalls.append(df_cm[col][col] / (df_cm[col][0] + df_cm[col][1] + df_cm[col][2] + df_cm[col][3]))\n",
    "    \n",
    "    total_sum = 0\n",
    "    overall_recall = 0\n",
    "    for col in range(4):\n",
    "        col_sum = 0\n",
    "        for row in range(4):\n",
    "            col_sum += df_cm[row][col]\n",
    "        total_sum += col_sum\n",
    "        overall_recall += recalls[col] * col_sum\n",
    "    recalls.append(overall_recall / total_sum)\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(precisions, recalls):\n",
    "    f1_scores = []\n",
    "    for i in range(5):\n",
    "        f1_scores[i] = 2 * (precisions[i] * recalls[i]) / (precisions[i] + recalls[i])\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\hline\n",
      " Variables   & Percentages   \\\\\n",
      "\\hline\n",
      " Variable 1  & 20\\%           \\\\\n",
      " Variable 2  & 35\\%           \\\\\n",
      " Variable 3  & 15\\%           \\\\\n",
      " Variable 4  & 10\\%           \\\\\n",
      " Variable 5  & 20\\%           \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Data for the table\n",
    "variables = ['Variable 1', 'Variable 2', 'Variable 3', 'Variable 4', 'Variable 5']\n",
    "percentages = ['20%', '35%', '15%', '10%', '20%']\n",
    "data = {'Variables': variables, 'Percentages': percentages}\n",
    "\n",
    "# Create a pandas DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to a LaTeX table\n",
    "table = tabulate(df, headers='keys', tablefmt='latex', showindex=False)\n",
    "\n",
    "# Print the LaTeX table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='privateuseone:0')\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='privateuseone:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_testing_data(model, criterion, device, eval=False):\n",
    "\n",
    "    ood_dataset = ImageFolder(root='DataOOD', transform= transforms.Compose([transforms.Grayscale()]))\n",
    "    ood_dataset = CustomDataset(ood_dataset, transform= get_transforms())\n",
    "    ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=hparams.test_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    pred_loss = 0\n",
    "    pred_correct = 0\n",
    "    total_size = 0\n",
    "\n",
    "    predictions = torch.IntTensor()\n",
    "    ground_truths = torch.IntTensor()\n",
    "\n",
    "    predictions, ground_truths = predictions.to(device), ground_truths.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (img, mean, std, target) in enumerate(ood_loader):\n",
    "            #img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n",
    "            #output = model(img, mean, std)\n",
    "            #print(output)\n",
    "        for batch_idx, (img, target) in enumerate(ood_loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, target)\n",
    "            pred_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            print(pred)\n",
    "            pred_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            predictions = torch.cat((predictions, pred), dim=0)\n",
    "            ground_truths = torch.cat((ground_truths, target), dim=0)\n",
    "\n",
    "            total_size += len(img)\n",
    "\n",
    "    pred_loss /= total_size\n",
    "    pred_accuracy = 100. * pred_correct / total_size\n",
    "\n",
    "    if eval:\n",
    "        return pred_loss, pred_accuracy, predictions.cpu().numpy(), ground_truths.cpu().numpy()\n",
    "    else:\n",
    "        return predictions.cpu().numpy(), ground_truths.cpu().numpy()\n",
    "    \n",
    "predict_testing_data(model, criterion, device, eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grad-cam in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (1.4.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (1.24.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (10.0.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (2.0.0)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (0.15.1)\n",
      "Requirement already satisfied: ttach in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (0.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (4.65.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (4.8.0.74)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from grad-cam) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from torch>=1.7.1->grad-cam) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from torch>=1.7.1->grad-cam) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from torch>=1.7.1->grad-cam) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from torch>=1.7.1->grad-cam) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from torch>=1.7.1->grad-cam) (3.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from torchvision>=0.8.2->grad-cam) (2.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from matplotlib->grad-cam) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from scikit-learn->grad-cam) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from scikit-learn->grad-cam) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from scikit-learn->grad-cam) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from tqdm->grad-cam) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from requests->torchvision>=0.8.2->grad-cam) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from requests->torchvision>=0.8.2->grad-cam) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from requests->torchvision>=0.8.2->grad-cam) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from requests->torchvision>=0.8.2->grad-cam) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hello\\onedrive\\desktop\\alzheimerresearch\\.venv\\lib\\site-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m rgb_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32(rgb_img) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[0;32m     22\u001b[0m cam \u001b[39m=\u001b[39m GradCAM(model\u001b[39m=\u001b[39mmodel, target_layers\u001b[39m=\u001b[39mtarget_layers, use_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 23\u001b[0m grayscale_cam \u001b[39m=\u001b[39m cam(input_tensor, targets\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     24\u001b[0m grayscale_cam \u001b[39m=\u001b[39m grayscale_cam[\u001b[39m0\u001b[39m, :]\n\u001b[0;32m     25\u001b[0m visualization \u001b[39m=\u001b[39m show_cam_on_image(rgb_img, grayscale_cam, use_rgb\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[0;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[0;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:95\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m     84\u001b[0m     loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m \u001b[39m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m# computed with a single target layer.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m# or something else.\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m cam_per_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_cam_per_layer(input_tensor,\n\u001b[0;32m     96\u001b[0m                                            targets,\n\u001b[0;32m     97\u001b[0m                                            eigen_smooth)\n\u001b[0;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:127\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(grads_list):\n\u001b[0;32m    125\u001b[0m     layer_grads \u001b[39m=\u001b[39m grads_list[i]\n\u001b[1;32m--> 127\u001b[0m cam \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_cam_image(input_tensor,\n\u001b[0;32m    128\u001b[0m                          target_layer,\n\u001b[0;32m    129\u001b[0m                          targets,\n\u001b[0;32m    130\u001b[0m                          layer_activations,\n\u001b[0;32m    131\u001b[0m                          layer_grads,\n\u001b[0;32m    132\u001b[0m                          eigen_smooth)\n\u001b[0;32m    133\u001b[0m cam \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(cam, \u001b[39m0\u001b[39m)\n\u001b[0;32m    134\u001b[0m scaled \u001b[39m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:50\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[1;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cam_image\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     43\u001b[0m                   input_tensor: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m     44\u001b[0m                   target_layer: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m                   grads: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m     48\u001b[0m                   eigen_smooth: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m---> 50\u001b[0m     weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_cam_weights(input_tensor,\n\u001b[0;32m     51\u001b[0m                                    target_layer,\n\u001b[0;32m     52\u001b[0m                                    targets,\n\u001b[0;32m     53\u001b[0m                                    activations,\n\u001b[0;32m     54\u001b[0m                                    grads)\n\u001b[0;32m     55\u001b[0m     weighted_activations \u001b[39m=\u001b[39m weights[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m activations\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m eigen_smooth:\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\pytorch_grad_cam\\grad_cam.py:22\u001b[0m, in \u001b[0;36mGradCAM.get_cam_weights\u001b[1;34m(self, input_tensor, target_layer, target_category, activations, grads)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cam_weights\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m     17\u001b[0m                     input_tensor,\n\u001b[0;32m     18\u001b[0m                     target_layer,\n\u001b[0;32m     19\u001b[0m                     target_category,\n\u001b[0;32m     20\u001b[0m                     activations,\n\u001b[0;32m     21\u001b[0m                     grads):\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mmean(grads, axis\u001b[39m=\u001b[39;49m(\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39m_mean(a, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   3465\u001b[0m                       out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:169\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    165\u001b[0m arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[0;32m    167\u001b[0m is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m umr_any(rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    171\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mMean of empty slice.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hello\\OneDrive\\Desktop\\AlzheimerResearch\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[0;32m     75\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     76\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m axis:\n\u001b[1;32m---> 77\u001b[0m         items \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mshape[mu\u001b[39m.\u001b[39;49mnormalize_axis_index(ax, arr\u001b[39m.\u001b[39;49mndim)]\n\u001b[0;32m     78\u001b[0m     items \u001b[39m=\u001b[39m nt\u001b[39m.\u001b[39mintp(items)\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[39m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m     \u001b[39m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "import cv2\n",
    "\n",
    "#model = resnet50(pretrained=True)\n",
    "model = CNN().to(device)\n",
    "path_to_pt_file = \"saved_model\\Model 20 (Very Good)\\CNN_2023_07_21-22_52_21.pt\"\n",
    "saved_model = torch.load(path_to_pt_file)\n",
    "model.load_state_dict(saved_model)\n",
    "target_layers = [list(model.modules())[10]] #change numbers //blue = model not focusing there //red = model is focusing there //error --> dropout layer\n",
    "path = 'Data\\Mild Dementia\\OAS1_0028_MR1_mpr-1_100.jpg'\n",
    "\n",
    "image = Image.open(path)\n",
    "image = image.convert('L')\n",
    "input_tensor = get_transforms()(image).unsqueeze(0)\n",
    "input_tensor = input_tensor.to(device)\n",
    "rgb_img = cv2.imread(path)\n",
    "rgb_img = cv2.resize(rgb_img, (248, 248))\n",
    "rgb_img = np.float32(rgb_img) / 255\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "grayscale_cam = cam(input_tensor, targets=None)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "cv2.imwrite(f'cam.png', visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
